{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BpHTkPEw5St"
      },
      "outputs": [],
      "source": [
        "# Boosting in Machine Learning - Jupyter Notebook Assignment\n",
        "\n",
        "## Theoretical Part\n",
        "\n",
        "### 1. What is Boosting in Machine Learning?\n",
        "Boosting is an ensemble learning technique that combines multiple weak learners (usually decision trees) to form a strong learner. It sequentially trains models, giving more weight to misclassified instances to improve overall performance.\n",
        "\n",
        "### 2. How does Boosting differ from Bagging?\n",
        "- **Boosting**: Models are trained sequentially, and each new model focuses on correcting the errors of previous models.\n",
        "- **Bagging**: Models are trained independently and in parallel, with results combined using averaging (regression) or voting (classification).\n",
        "\n",
        "### 3. What is the key idea behind AdaBoost?\n",
        "The key idea behind **AdaBoost (Adaptive Boosting)** is to assign weights to each training sample. Initially, all samples have equal weight, but misclassified samples get higher weights in the next iteration, making the model focus on difficult cases.\n",
        "\n",
        "### 4. Explain the working of AdaBoost with an example.\n",
        "- Start with equal weights for all training samples.\n",
        "- Train a weak classifier (e.g., decision stump).\n",
        "- Assign higher weights to misclassified samples.\n",
        "- Train the next classifier with updated weights.\n",
        "- Combine weak classifiers to make the final strong classifier.\n",
        "\n",
        "Example: Classifying emails as spam or not spam using decision stumps. Misclassified emails get higher weight, so the next iteration focuses on them.\n",
        "\n",
        "### 5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "Gradient Boosting improves predictions by optimizing a loss function. Unlike AdaBoost, which adjusts sample weights, Gradient Boosting minimizes errors by training models on the residual errors (differences between actual and predicted values).\n",
        "\n",
        "### 6. What is the loss function in Gradient Boosting?\n",
        "The loss function in Gradient Boosting depends on the problem type:\n",
        "- **Regression**: Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n",
        "- **Classification**: Log Loss (Cross-Entropy Loss).\n",
        "\n",
        "### 7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "XGBoost (Extreme Gradient Boosting) improves efficiency and performance with:\n",
        "- Regularization (L1 & L2)\n",
        "- Parallel processing\n",
        "- Handling missing values automatically\n",
        "- Pruning (stopping unnecessary tree growth)\n",
        "\n",
        "### 8. What is the difference between XGBoost and CatBoost?\n",
        "- **XGBoost**: Works well for structured data and numerical features.\n",
        "- **CatBoost**: Optimized for categorical data, using **ordered boosting** to prevent target leakage.\n",
        "\n",
        "### 9. What are some real-world applications of Boosting techniques?\n",
        "- Fraud detection (banking & finance)\n",
        "- Medical diagnosis (cancer detection)\n",
        "- Recommender systems (Netflix, Amazon)\n",
        "- Autonomous driving (object detection)\n",
        "\n",
        "### 10. How does regularization help in XGBoost?\n",
        "Regularization (L1 & L2) prevents overfitting by penalizing complex models and reducing unnecessary splits in decision trees.\n",
        "\n",
        "### 11. What are some hyperparameters to tune in Gradient Boosting models?\n",
        "- Learning rate\n",
        "- Number of estimators (trees)\n",
        "- Maximum depth of trees\n",
        "- Subsample ratio\n",
        "- Minimum child weight\n",
        "\n",
        "### 12. What is the concept of Feature Importance in Boosting?\n",
        "Feature importance measures how much each feature contributes to model predictions, helping in feature selection and model interpretability.\n",
        "\n",
        "### 13. Why is CatBoost efficient for categorical data?\n",
        "CatBoost efficiently handles categorical data by using **ordered boosting** and **feature encoding**, reducing target leakage and improving accuracy.\n",
        "\"\"\"\n",
        "\n",
        "# Practical Part - Boosting Implementation in Python\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score, f1_score, mean_squared_error, log_loss, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "from sklearn.datasets import load_breast_cancer, make_classification, make_regression\n",
        "\n",
        "# Train an AdaBoost Classifier\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "adaboost_clf = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "print(\"\\nAdaBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Train an AdaBoost Regressor\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "adaboost_reg = AdaBoostRegressor(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "adaboost_reg.fit(X_train, y_train)\n",
        "y_pred = adaboost_reg.predict(X_test)\n",
        "print(\"\\nAdaBoost Regressor MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "# Train a Gradient Boosting Classifier on Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "print(\"\\nGradient Boosting Classifier Accuracy:\", accuracy_score(y_test, gb_clf.predict(X_test)))\n",
        "\n",
        "# Feature Importance for Gradient Boosting Classifier\n",
        "plt.figure(figsize=(10,5))\n",
        "feature_importances = pd.Series(gb_clf.feature_importances_, index=cancer.feature_names)\n",
        "feature_importances.nlargest(10).plot(kind='barh')\n",
        "plt.title(\"Feature Importance - Gradient Boosting\")\n",
        "plt.show()\n",
        "\n",
        "# Train an XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "print(\"\\nXGBoost Classifier Accuracy:\", accuracy_score(y_test, xgb_clf.predict(X_test)))\n",
        "\n",
        "# Train a CatBoost Classifier\n",
        "cb_clf = cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n",
        "cb_clf.fit(X_train, y_train)\n",
        "print(\"\\nCatBoost Classifier F1-Score:\", f1_score(y_test, cb_clf.predict(X_test)))\n",
        "\n",
        "# Plot Confusion Matrix for CatBoost Classifier\n",
        "conf_matrix = confusion_matrix(y_test, cb_clf.predict(X_test))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, cb_clf.predict(X_test))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bU5A27xNzgGv",
        "outputId": "e81f7f0c-645a-4caa-c646-cd82c888e54c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'confusion_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f4d2b5193b8b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix - CatBoost Classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUBVc31azhO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}